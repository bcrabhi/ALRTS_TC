def upload_file_to_s3(spark: SparkSession, local_path: str, s3_dest: str) -> None:
    """
    Copies a local file (Excel, log, etc.) to S3 using Hadoop FileSystem API.
    Works on YARN because the driver node has local file access.
    """
    sc = spark.sparkContext
    jvm = sc._jvm
    hadoop_conf = sc._jsc.hadoopConfiguration()

    local_fs_path = jvm.org.apache.hadoop.fs.Path(f"file://{os.path.abspath(local_path)}")
    s3_fs_path = jvm.org.apache.hadoop.fs.Path(s3_dest)

    s3_fs = s3_fs_path.getFileSystem(hadoop_conf)

    logging.info(f"Uploading local file to S3: {local_path} → {s3_dest}")
    s3_fs.copyFromLocalFile(False, True, local_fs_path, s3_fs_path)
    logging.info(f"Upload complete: {s3_dest}")


def upload_file_to_s3(spark, local_path, s3_dest):
    ...
    s3_fs.copyFromLocalFile(False, True, local_fs_path, s3_fs_path)

s3_ac_excel = s3_path("output", cfg.s3_parquet_base, runday, cfg.excel_autocomplete_file)
upload_file_to_s3(spark, str(excel_path), s3_ac_excel)


local_log_file = cfg.logpath / log_file_name
    s3_log_dest = s3_path("logs", cfg.s3_parquet_base, log_file_name)
    logging.info(f"Uploading log file to S3: {local_log_file} → {s3_log_dest}")
    try:
        upload_file_to_s3(spark, str(local_log_file), s3_log_dest)
        logging.info(f"Log file uploaded to S3: {s3_log_dest}")
    except Exception as log_err:
        logging.warning(f"Failed to upload log file to S3: {log_err}")

s3_detail_excel = s3_path("output", cfg.s3_parquet_base, runday, f"pa_client360_detail_{runday}.xlsx")
upload_file_to_s3(spark, str(detail_excel_path), s3_detail_excel)

s3_pivot_excel = s3_path("output", cfg.s3_parquet_base, runday, cfg.excel_pivot_file)
upload_file_to_s3(spark, str(pivot_excel_path), s3_pivot_excel)

upload_file_to_s3(spark, str(local_log_file), s3_log_dest)


# completeness flag
df["found_missing"] = np.where(
    df.get("eventtimestamp_a").isna() | df.get("eventtimestamp").isna(),
    "Y",
    ""
)

# -----------------------------------------------------------------------------
# SAS delete condition:
# if isBusiness_p="true" and dec_tm_ge_pref_tm="Y" then delete;
# -----------------------------------------------------------------------------

required_cols = {"isbusiness_p", "dec_tm_ge_pref_tm"}

if required_cols.issubset(set(df.columns)):

    df = df[
        ~(
            (df["isbusiness_p"].astype(str).str.lower() == "true") &
            (df["dec_tm_ge_pref_tm"].astype(str).str.upper() == "Y")
        )
    ].copy()

# -----------------------------------------------------------------------------
# Base samples (18 per decision_date deterministic)
# -----------------------------------------------------------------------------

if "decision_date" not in df.columns:
    raise RuntimeError("decision_date column missing before sampling")

base_samples = deterministic_sample_18_per_day(
    df,
    "decision_date",
    seed=123
)

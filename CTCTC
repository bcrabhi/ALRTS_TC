#!/usr/bin/env python3

"""
C86 Alerts Cards â€“ Trino Memory Connector Version

Heavy processing (UNION, JOIN, DEDUPE) pushed to Trino.
Memory connector used like SAS WORK tables.
Pandas used only for SLA, sampling, aggregations, exports.
"""

from __future__ import annotations

import configparser
import logging
import os
import sys
from datetime import date
from pathlib import Path
from typing import Optional

import numpy as np
import pandas as pd
import trino
from trino.auth import BasicAuthentication


# =============================================================================
# CONFIG
# =============================================================================

def load_config(path: Path = Path("config.ini")) -> dict:
    cfg = configparser.ConfigParser()
    cfg.read(path)
    c = cfg["TRINO"]

    return {
        "host": c.get("host"),
        "port": c.getint("port"),
        "user": c.get("username"),
        "password": c.get("password", fallback=None),
        "http_schema": c.get("http_schema", fallback="https"),
        "catalog": c.get("catalog"),
        "schema": c.get("schema"),
        "source": c.get("source", fallback="c86_alerts_cards"),
        "out_root": c.get("out_root", fallback="./output"),
    }


def get_conn(conf: dict):
    auth = None
    if conf.get("password"):
        auth = BasicAuthentication(conf["user"], conf["password"])

    return trino.dbapi.connect(
        host=conf["host"],
        port=conf["port"],
        user=conf["user"],
        http_scheme=conf["http_schema"],
        auth=auth,
        catalog=conf["catalog"],
        schema=conf["schema"],
        source=conf["source"],
    )


def exec_sql(conn, sql: str):
    cur = conn.cursor()
    cur.execute(sql)
    return cur.fetchall()


def fetch_df(conn, sql: str) -> pd.DataFrame:
    cur = conn.cursor()
    cur.execute(sql)
    rows = cur.fetchall()
    cols = [c[0] for c in cur.description]
    return pd.DataFrame(rows, columns=cols)


# =============================================================================
# TRINO MEMORY TABLE BUILDERS
# =============================================================================

def build_memory_tables(conn):

    print("Dropping memory tables...")
    exec_sql(conn, "DROP TABLE IF EXISTS memory.default.cards_alert_final_m")
    exec_sql(conn, "DROP TABLE IF EXISTS memory.default.xb80_cards_m")
    exec_sql(conn, "DROP TABLE IF EXISTS memory.default.cards_dec_pref_m")
    exec_sql(conn, "DROP TABLE IF EXISTS memory.default.fft0_inbox_m")

    # -------------------------------------------------------------------------
    # xb80_cards_m
    # -------------------------------------------------------------------------

    exec_sql(conn, """
    CREATE TABLE memory.default.xb80_cards_m AS
    SELECT *
    FROM prod_brt0_ess.xb80_credit_card_system_interface
    WHERE event_activity_type = 'Alert Decision Cards'
      AND json_extract_scalar(eventAttributes['eventPayload'], '$.alertType')
            = 'AVAIL_CREDIT_REMAINING'
    """)

    # -------------------------------------------------------------------------
    # cards_dec_pref_m (union pushed to Trino)
    # -------------------------------------------------------------------------

    exec_sql(conn, """
    CREATE TABLE memory.default.cards_dec_pref_m AS
    SELECT *
    FROM (
        SELECT *
        FROM prod_brt0_ess.ffs_client_alert_preferences_dep_initial_load
        WHERE json_extract_scalar(eventAttributes['eventPayload'], '$.productType')
              = 'CREDIT_CARD'

        UNION ALL

        SELECT *
        FROM prod_brt0_ess.ffs_client_alert_preferences_dep
        WHERE json_extract_scalar(eventAttributes['eventPayload'], '$.productType')
              = 'CREDIT_CARD'
    ) t
    """)

    # -------------------------------------------------------------------------
    # fft0_inbox_m
    # -------------------------------------------------------------------------

    exec_sql(conn, """
    CREATE TABLE memory.default.fft0_inbox_m AS
    SELECT *
    FROM prod_brt0_ess.fft0_alert_inbox_dep
    WHERE event_activity_type = 'Alert Delivery Audit'
    """)

    # -------------------------------------------------------------------------
    # cards_alert_final_m (JOIN + DEDUPE in Trino)
    # -------------------------------------------------------------------------

    exec_sql(conn, """
    CREATE TABLE memory.default.cards_alert_final_m AS
    WITH joined AS (
        SELECT
            x.*,
            p.*,
            i.*,
            ROW_NUMBER() OVER (
                PARTITION BY json_extract_scalar(x.eventAttributes['eventPayload'],'$.decisionId')
                ORDER BY x.event_timestamp DESC
            ) AS rn
        FROM memory.default.xb80_cards_m x
        LEFT JOIN memory.default.cards_dec_pref_m p
            ON json_extract_scalar(x.eventAttributes['eventPayload'],'$.accountId')
               = json_extract_scalar(p.eventAttributes['eventPayload'],'$.externalAccount')
        LEFT JOIN memory.default.fft0_inbox_m i
            ON json_extract_scalar(x.eventAttributes['eventPayload'],'$.decisionId')
               = json_extract_scalar(i.eventAttributes['eventPayload'],'$.decisionId')
    )
    SELECT *
    FROM joined
    WHERE rn = 1
    """)

    print("Memory tables built.")


# =============================================================================
# PANDAS TRANSFORMS
# =============================================================================

def build_pandas_outputs(conn):

    df = fetch_df(conn, "SELECT * FROM memory.default.cards_alert_final_m")

    # -------------------------------------------------------------------------
    # SLA + bucket
    # -------------------------------------------------------------------------

    df["decision_ts"] = pd.to_datetime(df["event_timestamp"], errors="coerce")
    df["sent_ts"] = pd.to_datetime(df["event_timestamp_a"], errors="coerce")

    df["Time_Diff"] = (df["sent_ts"] - df["decision_ts"]).dt.total_seconds()

    df["SLA_Ind"] = np.where(df["Time_Diff"] <= 1800, "Y", "N")

    def bucket(sec):
        if pd.isna(sec):
            return "99 - Timestamp is missing"
        if sec <= 1800:
            return "01 - <= 30 mins"
        if sec <= 3600:
            return "02 - <= 60 mins"
        return "03 - > 60 mins"

    df["Alert_Time"] = df["Time_Diff"].apply(bucket)

    # -------------------------------------------------------------------------
    # Deterministic base samples (18 per decision_date)
    # -------------------------------------------------------------------------

    df["decision_date"] = df["decision_ts"].dt.date

    samples = []
    rng = np.random.default_rng(123)

    for d, g in df.groupby("decision_date"):
        k = min(18, len(g))
        idx = rng.choice(g.index, k, replace=False)
        samples.append(g.loc[idx])

    base_samples = pd.concat(samples)

    # -------------------------------------------------------------------------
    # Aggregations
    # -------------------------------------------------------------------------

    def agg(src, control, test):
        tmp = src.copy()
        tmp["ControlRisk"] = control
        tmp["TestType"] = test
        return tmp.groupby(["ControlRisk", "TestType"]).agg(
            Volume=("decision_ts", "size")
        ).reset_index()

    accuracy = agg(base_samples, "Accuracy", "Sample")
    timeliness = agg(df, "Timeliness", "Anomaly")
    completeness = agg(df[df["decisionId_a"].isna()], "Completeness", "Recon")

    ac_week = pd.concat([accuracy, timeliness, completeness])

    # -------------------------------------------------------------------------
    # Fail tables
    # -------------------------------------------------------------------------

    completeness_fail = df[df["decisionId_a"].isna()]
    timeliness_fail = df[df["SLA_Ind"] != "Y"]
    accuracy_fail = base_samples[base_samples["SLA_Ind"] != "Y"]

    return ac_week, completeness_fail, timeliness_fail, accuracy_fail


# =============================================================================
# MAIN
# =============================================================================

def main():
    conf = load_config()
    conn = get_conn(conf)

    build_memory_tables(conn)

    ac_week, comp_fail, time_fail, acc_fail = build_pandas_outputs(conn)

    out = Path(conf["out_root"])
    out.mkdir(parents=True, exist_ok=True)

    ac_week.to_excel(out / "Alert_Cards_AC_week.xlsx", index=False)
    comp_fail.to_excel(out / "Completeness_Fail.xlsx", index=False)
    time_fail.to_excel(out / "Timeliness_Fail.xlsx", index=False)
    acc_fail.to_excel(out / "Accuracy_Fail.xlsx", index=False)

    print("DONE")


if __name__ == "__main__":
    main()
